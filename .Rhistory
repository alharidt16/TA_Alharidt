install.packages(c("plumber", "callr", "httr"))
install.packages(c("plumber", "callr", "httr"))
install.packages(c("plumber", "callr", "httr"))
install.packages("mlflow")
install.packages("mlflow")
devtools::install_github("rstudio/mleap")
install.packages(c("plumber", "callr", "httr"))
install.packages(c("plumber", "callr", "httr"))
install.packages("mlflow")
install.packages("mlflow")
devtools::install_github("rstudio/mleap")
mleap::install_maven()
install.packages("mleap")
install.packages("mleap")
mleap::install_maven()
mleap::install_mleap()
mlflow::install_mlflow()
install.packages("plumber")
install.packages("callr")
install.packages("httr")
install.packages("mlflow")
install.packages("mlflow")
install.packages("mleap")
install.packages("mleap")
mleap::install_maven()
mleap::install_mleap()
devtools::install_github("rstudio/mleap")
install.packages("mleap")
mleap::install_maven()
library(mleap)
mleap::install_maven()
library(mleap)
library(sparklyr)
library(dplyr)
library(ggplot2)
# library(arrow)
conf <- spark_config()
conf$`sparklyr.shell.driver-memory` <- "4G"
# CURRENTLY ONLY spark 2.3 is supported for this part.
# install if you do not already have it
# spark_install(version = "2.3.3")
spark <- spark_connect(master = "local", version = "2.3.3", config= conf)
iris_tbl <- copy_to(spark, iris, "iris", overwrite = TRUE)
head(iris_tbl)
kmeans_model <- iris_tbl %>%
select(Petal_Width, Petal_Length) %>%
ml_kmeans(~ ., k = 3)
kmeans_model
# predict the associated class
predicted <- ml_predict(kmeans_model, iris_tbl) %>%
collect
base::table(predicted$Species, predicted$prediction)
# plot cluster membership
ml_predict(kmeans_model) %>%
collect() %>%
ggplot(aes(Petal_Length, Petal_Width)) +
geom_point(aes(Petal_Width, Petal_Length, col = factor(prediction + 1)),
size = 2, alpha = 0.5) +
geom_point(data = kmeans_model$centers, aes(Petal_Width, Petal_Length),
col = scales::muted(c("red", "green", "blue")),
pch = 'x', size = 12) +
scale_color_discrete(name = "Predicted Cluster",
labels = paste("Cluster", 1:3)) +
labs(
x = "Petal Length",
y = "Petal Width",
title = "K-Means Clustering",
subtitle = "Use Spark.ML to predict cluster membership with the iris dataset."
)
movies <- data.frame(user   = c(1, 2, 0, 1, 2, 0),
item   = c(1, 1, 1, 2, 2, 0),
rating = c(3, 1, 2, 4, 5, 4))
copy_to(spark, movies) %>%
ml_als(rating ~ user + item) %>%
augment()
mtcars_tbl <- copy_to(spark, mtcars, "mtcars")
# transform our data set, and then partition into 'training', 'test'
partitions <- mtcars_tbl %>%
filter(hp >= 100) %>%
ft_bucketizer(input_col  = "cyl",
output_col = "cyl8",
splits     = c(0,8,12)) %>%
sdf_random_split(training = 0.5, test = 0.5, seed = 888)
# fit a linear mdoel to the training dataset
fit <- partitions$training %>%
ml_linear_regression(mpg ~ wt + cyl)
# summarize the model
summary(fit)
# Score the data
pred <- ml_predict(fit, partitions$test) %>%
collect
# Plot the predicted versus actual mpg
ggplot(pred, aes(x = mpg, y = prediction)) +
geom_abline(lty = "dashed", col = "red") +
geom_point() +
theme(plot.title = element_text(hjust = 0.5)) +
coord_fixed(ratio = 1) +
labs(
x = "Actual Fuel Consumption",
y = "Predicted Fuel Consumption",
title = "Predicted vs. Actual Fuel Consumption"
)
scaler <- ft_standard_scaler(
spark,
input_col = "features",
output_col = "features_scaled",
with_mean = TRUE)
df <- copy_to(spark, data.frame(value = rnorm(100000))) %>%
ft_vector_assembler(input_cols = "value", output_col = "features")
scaler_model <- ml_fit(scaler, df)
# look at the model. Nicle it prints the different stages
scaler_model
scaler_model %>%
ml_transform(df) %>%
glimpse()
spark_disconnect(spark)
devtools::install_github("rstudio/graphframes")
install.packages("ggraph")
library(ggraph)
library(graphframes)
library(sparklyr)
library(dplyr)
conf <- spark_config()
conf$`sparklyr.shell.driver-memory` <- "4G"
spark <- spark_connect(master = "local", config = conf)
library(sparklyr)
library(dplyr)
conf <- spark_config()
conf$`sparklyr.shell.driver-memory` <- "4G"
spark <- spark_connect(master = "local", config = conf)
spark <- spark_connect(master = "local", version = "2.4.8", config = conf)
highschool_tbl <- copy_to(spark, ggraph::highschool, "highschool")
highschool_tbl
highschool_tbl <- copy_to(spark, ggraph::highschool, "highschool", overwrite = TRUE) %>%
filter(year == 1957) %>%
transmute(from = as.character(as.integer(from)),
to = as.character(as.integer(to)))
from_tbl <- highschool_tbl %>%
distinct(from) %>%
transmute(id = from)
to_tbl <- highschool_tbl %>%
distinct(to) %>%
transmute(id = to)
vertices_tbl <- distinct(sdf_bind_rows(from_tbl, to_tbl))
edges_tbl <- highschool_tbl %>%
transmute(src = from, dst = to)
vertices_tbl
edges_tbl
graph <- gf_graphframe(vertices_tbl, edges_tbl)
graph
gf_degrees(graph) %>% summarise(friends = mean(degree))
gf_shortest_paths(graph, 33) %>%
filter(size(distances) > 0) %>%
mutate(distance = explode(map_values(distances))) %>%
select(id, distance)
gf_graphframe(vertices_tbl, edges_tbl) %>%
gf_pagerank(reset_prob = 0.15, max_iter = 10L)
highschool_tbl %>%
igraph::graph_from_data_frame(directed = FALSE) %>%
ggraph(layout = 'kk') +
geom_edge_link(alpha = 0.2,
arrow = arrow(length = unit(2, 'mm')),
end_cap = circle(2, 'mm'),
start_cap = circle(2, 'mm')) +
geom_node_point(size = 2, alpha = 0.4)
spark_disconnect(spark)
library(sparklyr)
spark_installed_versions()
system("java -version")
library(devtools)
packageVersion("sparklyr")
library(sparklyr)
library(sparklyr)
spark_available_versions()
spark_installed_versions()
suppressPackageStartupMessages(library("dplyr"))
spark <- spark_connect(master = "local")
spark_disconnect(spark)
library(sparklyr)
conf <- spark_config()
conf$`sparklyr.shell.driver-memory` <- "4G"
spark <- spark_connect(master = "local", config = conf)
spark_disconnect(spark)
library(sparklyr)
library(dplyr)
library(arrow)
conf <- spark_config()
conf$`sparklyr.shell.driver-memory` <- "4G"
spark <- spark_connect(master = "local", config = conf)
pagecountsAllDF <- spark_read_parquet(spark, "data/wikimedia_edits.parquet")
sdf_nrow(pagecountsAllDF)
sdf_nrow(pagecountsAllDF)
sdf_schema(pagecountsAllDF)
pagecountsAllDF %>%
select(project) %>%
distinct()
pagecountsAllDF %>%
group_by(project)%>%
summarise(count_per_project = count())
# dplyr exact
pagecountsAllDF %>%
group_by(project)%>%
summarise(count_unique = n_distinct(article))
# dplyr approx
pagecountsAllDF %>%
group_by(project)%>%
summarise(count_unique = approx_count_distinct(article))
pagecountsAllDF
# be careful with what you collect locally
local_df <- sparklyr::collect(pagecountsAllDF)
transformedDf <- pagecountsAllDF %>%
filter(requests == 1)
transformedDf
pagecountsAllDF %>%
select(requests, bytes_served)
pagecountsAllDF %>%
select(-requests, -article)
pagecountsAllDF %>%
distinct(requests)
pagecountsAllDF %>%
arrange(requests)
pagecountsAllDF %>%
arrange(desc(requests))
library(nycflights13)
flights <- copy_to(spark, flights, "flights")
airlines <- copy_to(spark, airlines, "airlines")
bestworst <- flights %>%
group_by(year, month, day) %>%
select(dep_delay) %>%
filter(dep_delay == min(dep_delay) || dep_delay == max(dep_delay))
dbplyr::sql_render(bestworst)
bestworst
flights %>% left_join(airlines)
spark_write_parquet(flights, "data/flights" , mode="overwrite", options = list(compression="gzip"))
spark_write_parquet(flights, "data/flights" ,partition_by = "carrier", mode="overwrite", options = list(compression="gzip"))
initialDf <- spark_read_csv(spark, "data/green_tripdata_2018-01.csv.gz", infer_schema = FALSE)
initialDf <- spark_read_csv(spark, "data/green_tripdata_2018-01.csv.gz", infer_schema = TRUE)
# as this is slow
# in a real world scenario things will be structured in databases
# sparklyr::src_databases(spark) # or your
# tbl_change_db(<<m<_db>>)
src_tbls(spark)
# cache it once so interactive steps later are faster
#tbl_cache(spark, "green_tripdata_201801_csv")
renamed <- initialDf %>%
select(extra) %>%
rename(extra_things = "extra")
sdf_schema(renamed)
initialDf %>%
select (lpep_dropoff_datetime) %>%
mutate(today = current_date()) %>% # notice we can use Hive UDF
mutate(currentTc = as.integer(year(lpep_dropoff_datetime))) %>% # and also cast
mutate(useless_tc = date_format(lpep_dropoff_datetime, "yyyy-dd")) # and also castthe data type
flights %>%
summarise(mean_dep_delay = mean(dep_delay))
delayPerCarrier <- flights %>%
group_by(carrier) %>%
summarise(mean_dep_delay = mean(dep_delay),
data=sort_array(collect_set(dep_delay))) # note also less common aggregate functions are available
delayPerCarrier
delay <- flights %>% group_by(tailnum) %>% summarise(
count = n(),
dist = mean(distance),
delay = mean(arr_delay)
) %>% filter(count > 20,
dist <
2000,!is.na(delay)) %>% collect()
head(delay)
library(ggplot2)
ggplot(
delay,aes(dist,delay))+
geom_point(aes(size=count),alpha=1/2)+ geom_smooth()+ scale_size_area(max_size=2)
# First, let's look at the nested data.
# Unofrtunately, it is not nicely viewable from RStudio
# now when taking the first 5 rows into local R memory and examining the list:
delayPerCarrierTransformed <- delayPerCarrier %>%
ft_sql_transformer("
SELECT carrier,
mean_dep_delay,
data,
TRANSFORM(data, element -> element +1) AS data_plus,
FILTER(data, element -> element <5) AS filtered_data
FROM   __THIS__")
delayPerCarrierTransformed
local_delays_after <- delayPerCarrierTransformed %>%
head(n=6) %>%
collect
# before
local_delays_after$data[[1]][1]
# afte
local_delays_after$data_plus[[1]][1]
# take a look at the filtered data
# do.call(rbind, local_delays_after$data)
# do.call(rbind, local_delays_after$data_plus)
do.call(rbind, local_delays_after$filtered_data)
# library(sparklyr.nested)
# delayPerCarrierTransformed %>% sdf_explode(data_plus)
initialDf %>%
group_by(date_format(lpep_pickup_datetime,'E')) %>%
count()
spark_disconnect(spark)
library(sparklyr)
library(dplyr)
library(arrow)
conf <- spark_config()
conf$`sparklyr.shell.driver-memory` <- "4G"
spark <- spark_connect(master = "local", config = conf)
pagecountsAllDF <- spark_read_parquet(spark, "data/wikimedia_edits.parquet")
spark_installed_versions()
spark <- spark_connect(master = "local")
spark_disconnect(spark)
spark <- spark_connect(master = "local")
spark_install(version = "2.4.8")
spark_disconnect(spark)
library(sparklyr)
library(devtools)
library(devtools)
devtools::install_github("harryprince/geospark")
packageVersion("sparklyr")
library(sparklyr)
spark_available_versions()
spark_installed_versions()
spark <- spark_connect(master = "local")
spark_disconnect(spark)
library(sparklyr)
conf <- spark_config()
conf$`sparklyr.shell.driver-memory` <- "4G"
spark <- spark_connect(master = "local", config = conf)
spark_disconnect(spark)
library(sparklyr)
library(dplyr)
library(arrow)
conf <- spark_config()
conf$`sparklyr.shell.driver-memory` <- "4G"
spark <- spark_connect(master = "local", config = conf)
pagecountsAllDF <- spark_read_parquet(spark, "data/wikimedia_edits.parquet")
sdf_nrow(pagecountsAllDF)
sdf_nrow(pagecountsAllDF)
sdf_schema(pagecountsAllDF)
pagecountsAllDF %>%
select(project) %>%
distinct()
pagecountsAllDF %>%
group_by(project)%>%
summarise(count_per_project = count())
# dplyr exact
pagecountsAllDF %>%
group_by(project)%>%
summarise(count_unique = n_distinct(article))
# dplyr approx
pagecountsAllDF %>%
group_by(project)%>%
summarise(count_unique = approx_count_distinct(article))
pagecountsAllDF
# be careful with what you collect locally
local_df <- sparklyr::collect(pagecountsAllDF)
transformedDf <- pagecountsAllDF %>%
filter(requests == 1)
transformedDf
pagecountsAllDF %>%
select(requests, bytes_served)
pagecountsAllDF %>%
select(-requests, -article)
pagecountsAllDF %>%
distinct(requests)
pagecountsAllDF %>%
arrange(requests)
pagecountsAllDF %>%
arrange(desc(requests))
library(nycflights13)
flights <- copy_to(spark, flights, "flights")
airlines <- copy_to(spark, airlines, "airlines")
bestworst <- flights %>%
group_by(year, month, day) %>%
select(dep_delay) %>%
filter(dep_delay == min(dep_delay) || dep_delay == max(dep_delay))
dbplyr::sql_render(bestworst)
bestworst
flights %>% left_join(airlines)
spark_write_parquet(flights, "data/flights" , mode="overwrite", options = list(compression="gzip"))
spark_write_parquet(flights, "data/flights" ,partition_by = "carrier", mode="overwrite", options = list(compression="gzip"))
initialDf <- spark_read_csv(spark, "data/green_tripdata_2018-01.csv.gz", infer_schema = FALSE)
initialDf <- spark_read_csv(spark, "data/green_tripdata_2018-01.csv.gz", infer_schema = TRUE)
# as this is slow
# in a real world scenario things will be structured in databases
# sparklyr::src_databases(spark) # or your
# tbl_change_db(<<m<_db>>)
src_tbls(spark)
# cache it once so interactive steps later are faster
#tbl_cache(spark, "green_tripdata_201801_csv")
renamed <- initialDf %>%
select(extra) %>%
rename(extra_things = "extra")
sdf_schema(renamed)
initialDf %>%
select (lpep_dropoff_datetime) %>%
mutate(today = current_date()) %>% # notice we can use Hive UDF
mutate(currentTc = as.integer(year(lpep_dropoff_datetime))) %>% # and also cast
mutate(useless_tc = date_format(lpep_dropoff_datetime, "yyyy-dd")) # and also castthe data type
flights %>%
summarise(mean_dep_delay = mean(dep_delay))
delayPerCarrier <- flights %>%
group_by(carrier) %>%
summarise(mean_dep_delay = mean(dep_delay),
data=sort_array(collect_set(dep_delay))) # note also less common aggregate functions are available
delayPerCarrier
delay <- flights %>% group_by(tailnum) %>% summarise(
count = n(),
dist = mean(distance),
delay = mean(arr_delay)
) %>% filter(count > 20,
dist <
2000,!is.na(delay)) %>% collect()
head(delay)
library(ggplot2)
ggplot(
delay,aes(dist,delay))+
geom_point(aes(size=count),alpha=1/2)+ geom_smooth()+ scale_size_area(max_size=2)
# First, let's look at the nested data.
# Unofrtunately, it is not nicely viewable from RStudio
# now when taking the first 5 rows into local R memory and examining the list:
delayPerCarrierTransformed <- delayPerCarrier %>%
ft_sql_transformer("
SELECT carrier,
mean_dep_delay,
data,
TRANSFORM(data, element -> element +1) AS data_plus,
FILTER(data, element -> element <5) AS filtered_data
FROM   __THIS__")
delayPerCarrierTransformed
local_delays_after <- delayPerCarrierTransformed %>%
head(n=6) %>%
collect
# before
local_delays_after$data[[1]][1]
# afte
local_delays_after$data_plus[[1]][1]
# take a look at the filtered data
# do.call(rbind, local_delays_after$data)
# do.call(rbind, local_delays_after$data_plus)
do.call(rbind, local_delays_after$filtered_data)
# library(sparklyr.nested)
# delayPerCarrierTransformed %>% sdf_explode(data_plus)
initialDf %>%
group_by(date_format(lpep_pickup_datetime,'E')) %>%
count()
system("java -version")
getwd()
pagecountsAllDF <- spark_read_parquet(spark, "data/wikimedia_edits.parquet")
spark_disconnect(spark)
library(sparklyr)
library(dplyr)
library(arrow)
conf <- spark_config()
conf$`sparklyr.shell.driver-memory` <- "4G"
spark <- spark_connect(master = "local", config = conf)
pagecountsAllDF <- spark_read_parquet(spark, "data/wikimedia_edits.parquet")
pagecountsAllDF <- spark_read_parquet(spark, "data/wikimedia_edits.parquet")
sdf_nrow(pagecountsAllDF)
sdf_nrow(pagecountsAllDF)
sdf_schema(pagecountsAllDF)
pagecountsAllDF %>%
select(project) %>%
distinct()
pagecountsAllDF %>%
group_by(project)%>%
summarise(count_per_project = count())
# dplyr exact
pagecountsAllDF %>%
group_by(project)%>%
summarise(count_unique = n_distinct(article))
# dplyr approx
pagecountsAllDF %>%
group_by(project)%>%
summarise(count_unique = approx_count_distinct(article))
pagecountsAllDF
# be careful with what you collect locally
local_df <- sparklyr::collect(pagecountsAllDF)
transformedDf <- pagecountsAllDF %>%
filter(requests == 1)
transformedDf
pagecountsAllDF %>%
select(requests, bytes_served)
pagecountsAllDF %>%
select(-requests, -article)
pagecountsAllDF %>%
distinct(requests)
pagecountsAllDF %>%
arrange(requests)
pagecountsAllDF %>%
arrange(desc(requests))
library(nycflights13)
flights <- copy_to(spark, flights, "flights")
spark_disconnect(spark)
getwd()
setwd("C:\Users\Alharidt\Downloads")
setwd("C:/Users/Alharidt/Downloads")
getwd()
setwd("D:/")
getwd()
getwd()
set("C:/Users/Alharidt/Documents/BIG DATA/18-10-2023")
setwd("C:/Users/Alharidt/Documents/BIG DATA/18-10-2023")
getwd()
data=read.csv("data/pokemon_raw.csv")
data=read.csv("data/pokemon_raw.csv",sept=',')
setwd("C:/Users/Alharidt/Documents/GitHub/TA_Alharidt")
getwd()
data=read.csv("Data_gabung _filter 2.csv")
data
rm(data)
View(local_delays_after)
